{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as torch_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing articles and vector dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('final_dump_700k.p', 'rb') as fp:\n",
    "    articles= pickle.load(fp)\n",
    "\n",
    "with open('google_dict_700k.p', 'rb') as fp:\n",
    "    embed= pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec=embed[0]\n",
    "idx2word=embed[1]\n",
    "word2idx=embed[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs=[]\n",
    "for i in range(len(articles['desc'])):\n",
    "    pairs.append((articles['desc'][i],articles['head'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Encoder Parameters and Foward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, LSTM_layers=4):\n",
    "        super(encoder, self).__init__()\n",
    "        self.LSTM_layers = LSTM_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding= nn.Embedding(input_size, hidden_size)\n",
    "        # pretrained_weight is a numpy matrix of shape (num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(wordvec))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size)).cuda()\n",
    "        return result\n",
    "\n",
    "    def forward(self, input_word, hidden):\n",
    "        output= self.embedding(input_word).view(1,1,-1)\n",
    "        # LSTM execution\n",
    "        for i in range(self.LSTM_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Decoder Parameters and Foward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, LSTM_layers=4):\n",
    "        super(decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.LSTM_layers = LSTM_layers\n",
    "        self.embedding= nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # pretrained_weight is a numpy matrix of shape (num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(wordvec))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        # max length of input or output=50\n",
    "        self.local_attn = nn.Linear(self.hidden_size*2, 50)\n",
    "        self.global_attn = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size)).cuda()\n",
    "        return result\n",
    "\n",
    "    def forward(self, input, hidden, encoder_output, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        weights = func_torch.softmax(local_attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
    "        attn = torch.bmm(weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "        output = torch.cat((embedded[0], attn[0]), 1)\n",
    "        output = self.global_attn(output).unsqueeze(0)\n",
    "\n",
    "        for i in range(self.LSTM_layers):\n",
    "            # LSTM execution\n",
    "            output = func_torch.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = torch_func.log_softmax(self.out(output[0]))\n",
    "        return output, hidden, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to create sentence vecs from word vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sentence_vecs(sentence):\n",
    "    sen=[]\n",
    "    for word in sentence:\n",
    "        if word in word2idx:\n",
    "            sen.append(word2idx[word])\n",
    "        else:\n",
    "            sen.append(word2idx[word.lower()])\n",
    "    return sen        \n",
    "\n",
    "\n",
    "def inputoutput_pair(pair):\n",
    "    desc_indexes = sentence_vecs(pair[0])\n",
    "    desc_indexes.append(1)\n",
    "    desc = Variable(torch.LongTensor(desc_indexes).view(-1, 1)).cuda()\n",
    "    head_indexes = sentence_vecs(pair[1])\n",
    "    head_indexes.append(1)\n",
    "    head = Variable(torch.LongTensor(head_indexes).view(-1, 1)).cuda() \n",
    "    return desc,head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining One train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(desc_senvec, head_senvec, encoder, decoder, encode_optim, decode_optim, loss_criteria):\n",
    "    max_length=50\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    decoder_hidden = encoder_hidden\n",
    "   \n",
    "    encode_optim.zero_grad()\n",
    "    decode_optim.zero_grad()\n",
    "    \n",
    "    desc_length = desc_senvec.size()[0]\n",
    "    head_length = head_senvec.size()[0]\n",
    "    \n",
    "    \n",
    "    collect_encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size)).cuda()\n",
    "    decoder_input = Variable(torch.LongTensor([[0]])).cuda()\n",
    "    \n",
    "\n",
    "    loss = 0\n",
    "    #pass through encoder\n",
    "\n",
    "    for i in range(desc_length):\n",
    "        #previous hidden state to passed to next\n",
    "        encoder_output, encoder_hidden = encoder(desc_senvec[i], encoder_hidden)\n",
    "        collect_encoder_outputs[i] = encoder_output[0][0]\n",
    "\n",
    "   \n",
    "    force_teach_ratio=0.5\n",
    "    force_teach = True if random.random() < force_teach_ratio else False\n",
    "    #pass through decoder \n",
    "    if force_teach:\n",
    "        # Feed imputs from headline itself\n",
    "        for i in range(head_length):\n",
    "            #previous hidden state to passed to next\n",
    "            decoder_output, decoder_hidden, attention = decoder(decoder_input, decoder_hidden, encoder_output, collect_encoder_outputs)\n",
    "            loss += loss_criteria(decoder_output,head_senvec[i])\n",
    "            decoder_input = head_senvec[i]\n",
    "\n",
    "    else:\n",
    "        # use its own prediction\n",
    "        for i in range(head_length):\n",
    "            #previous hidden state to passed to next\n",
    "            decoder_output, decoder_hidden, attention = decoder(decoder_input, decoder_hidden, encoder_output, collect_encoder_outputs)\n",
    "            top, indicies = decoder_output.data.topk(1)\n",
    "            top_word = indicies[0][0]\n",
    "            decoder_input = Variable(torch.LongTensor([[top_word]])).cuda()\n",
    "            loss += loss_criteria(decoder_output,head_senvec[i])\n",
    "            if top_word == 1:\n",
    "                break\n",
    "\n",
    "\n",
    "    loss.backward(retain_graph=False)\n",
    "    encode_optim.step()\n",
    "    decode_optim.step()\n",
    "\n",
    "\n",
    "    return loss.data[0]/head_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(encoder, decoder):\n",
    "    collected_loss = 0\n",
    "    \n",
    "    #filtering out variables that do not require grad\n",
    "    encoder_parameters= filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "    decoder_parameters= filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "    encode_optim = optim.SGD(encoder_parameters, lr=0.01)\n",
    "    decode_optim = optim.SGD(decoder_parameters, lr=0.01)\n",
    "    all_training_pairs = [inputoutput_pair(random.choice(pairs))for i in range(9*len(pairs))]\n",
    "    loss_criteria = nn.NLLLoss()\n",
    "    \n",
    "    \n",
    "    for i in range(len(pairs)):\n",
    "        print(i)\n",
    "        training_pair = all_training_pairs[i]\n",
    "        desc_senvec = training_pair[0]\n",
    "        head_senvec = training_pair[1]\n",
    "        loss_data = step(desc_senvec, head_senvec, encoder,decoder, encode_optim, decode_optim, loss_criteria)\n",
    "        collected_loss += loss_data\n",
    "        gc.collect()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            #saving model outputs & after every 1000 iterations\n",
    "            print_loss_avg = collected_loss / print_every\n",
    "            collected_loss = 0\n",
    "            torch.save(encoder.state_dict(), 'modelparam_encode.pkl')\n",
    "            torch.save(decoder.state_dict(), 'modelparam_decode.pkl')\n",
    "            boo=predict10(encoder1, decoder1)\n",
    "            filename='output_'+str(i)+'.txt'\n",
    "            thefile = open(filename, 'w',encoding=\"utf-8\")\n",
    "            for item in boo:\n",
    "                thefile.write(\"%s\\n\" %item)\n",
    "            thefile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict(encoder, decoder, sentence):\n",
    "    max_length=50\n",
    "    desc_vec = inputoutput_pair(sentence)\n",
    "    desc_length = desc_vec.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    decoder_hidden = encoder_hidden\n",
    "    collect_encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size)).cuda()\n",
    "    decoder_input = Variable(torch.LongTensor([[0]])).cuda()\n",
    "    decoded_sentence = []\n",
    "    #pass through encoder\n",
    "    for i in range(input_length):\n",
    "        #previous hidden state to passed to next\n",
    "        encoder_output, encoder_hidden = encoder(desc_vec[i],encoder_hidden)\n",
    "        collect_encoder_outputs[i] = collect_encoder_outputs[i] + encoder_output[0][0]\n",
    "\n",
    "    #pass through decoder\n",
    "    for i in range(max_length):\n",
    "        #previous hidden state to passed to next\n",
    "        decoder_output, decoder_hidden, attention = decoder(decoder_input, decoder_hidden, encoder_output, collect_encoder_outputs)\n",
    "        decoder_attentions[i] = attention.data\n",
    "        top,indices = decoder_output.data.topk(1)\n",
    "        top_word = indicies[0][0]\n",
    "        if top_word == 1:\n",
    "            decoded_sentence.append(idx2word[top_word])\n",
    "            break\n",
    "        else:\n",
    "            decoded_sentence.append(idx2word[top_word])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[top_word]])).cuda\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict10(encoder, decoder):\n",
    "    lst=[]\n",
    "    string=''\n",
    "    for i in range(10):\n",
    "        pair = random.choice(pairs)\n",
    "        output_words, attentions = predict(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        string='desc='+str(pair[0])+'/n'+'head='+str(pair[1])+'/n'+'pred_head='+str(output_sentence)\n",
    "        lst.append(string)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Encoder and Decoder objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "hidden_size = 300\n",
    "encoder1 = encoder(len(word2idx), hidden_size).cuda()\n",
    "decoder1 = decoder(hidden_size, len(word2idx)).cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(encoder1,decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(10, 20)\n",
    "input = Variable(torch.randn(5, 3, 10))\n",
    "h0 = Variable(torch.randn(2, 3, 20))\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "   0.0398 -1.3149  0.0819  0.8514  0.1276 -0.2468 -0.0910  1.2570 -1.5970\n",
       " -0.7522 -0.2079 -0.1343  1.7270  1.4326 -0.4948  0.9084  0.0818 -0.0658\n",
       " -0.0558  1.8751 -0.6088 -1.3149 -0.3310  0.3425 -0.2404  1.5707 -1.5710\n",
       "\n",
       "Columns 9 to 9 \n",
       "  -0.3240\n",
       " -1.4660\n",
       "  0.4556\n",
       "\n",
       "(1 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "   1.5445  0.1774  0.9916 -0.9857 -0.1012  1.6755  0.5544  0.9926 -0.5257\n",
       "  1.9864 -1.5486 -1.4686  1.1766 -1.0268 -2.1516  1.2332  0.7529  1.6097\n",
       " -0.9007 -0.6226 -0.8754 -0.0749  1.4322  1.2692  0.8320 -0.0702  0.0733\n",
       "\n",
       "Columns 9 to 9 \n",
       "   1.3429\n",
       " -0.5218\n",
       "  0.6132\n",
       "\n",
       "(2 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "  -0.5706  1.1447 -0.8833 -0.0575 -1.0513 -0.0103 -0.1368 -1.4337 -0.9689\n",
       " -1.5054 -0.3881  1.4895  0.5522  0.9423 -0.2670  0.0664 -0.1620  0.7033\n",
       "  0.8828  1.1705 -1.5171 -0.8225 -0.3020  0.9443 -0.0591 -0.7420 -0.3505\n",
       "\n",
       "Columns 9 to 9 \n",
       "  -0.2125\n",
       "  0.7226\n",
       " -1.7516\n",
       "\n",
       "(3 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "  -0.0306 -0.2640 -1.0900 -0.4419  0.5932 -1.3252 -0.3590  1.0630  0.3780\n",
       "  0.0855 -0.5175  0.7143  0.6035  0.4806 -0.4262  0.2257 -1.4840  0.2113\n",
       "  0.3362  1.2627 -1.3817 -0.7805  0.9752  1.0036  0.5102  2.6892 -0.1796\n",
       "\n",
       "Columns 9 to 9 \n",
       "   0.5537\n",
       " -1.9052\n",
       " -0.2111\n",
       "\n",
       "(4 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "   2.3827 -0.4468  0.5209 -1.6674 -2.1569 -0.9363  1.0418 -0.3417  1.2764\n",
       " -0.6167  0.3445 -1.2622 -0.9714 -0.6227 -0.4783  0.0570  1.5283 -0.6516\n",
       " -0.1353 -0.5683  1.8059 -0.8104 -2.2481  0.3892 -0.3379 -0.4549 -1.4117\n",
       "\n",
       "Columns 9 to 9 \n",
       "  -0.3731\n",
       " -0.5108\n",
       " -1.5134\n",
       "[torch.FloatTensor of size 5x3x10]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "   0.6530 -0.4791  0.5521 -0.0990  0.1123 -0.3781 -0.3385  0.2021 -0.6749\n",
       "  0.4998  0.3414 -1.4040  0.2471 -0.0622  0.4067 -1.9111 -0.8620 -0.1318\n",
       " -0.7250  1.2723  0.1659  0.4591  0.4363  0.0604  0.5597  1.3388 -0.3013\n",
       "\n",
       "Columns 9 to 17 \n",
       "   1.2496  0.2156  0.0829  0.7147  0.5354  0.7639 -0.1024  0.2023 -0.1542\n",
       "  0.7490 -0.6068 -0.0181  0.2090 -0.9057  0.6500  0.2627  0.1951 -0.4207\n",
       " -0.9079 -0.1000  0.3947 -0.5597 -0.4395 -0.0480 -0.5361 -0.3612  0.1380\n",
       "\n",
       "Columns 18 to 19 \n",
       "   0.2900 -0.0582\n",
       "  0.4374 -0.7356\n",
       "  0.1005  0.1004\n",
       "\n",
       "(1 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "   0.6957 -0.1143  0.3710  0.0560  0.1116 -0.2361  0.2654 -0.1698 -0.4823\n",
       "  0.0020 -0.3211 -0.7819  0.1158 -0.2284  0.4594 -1.1712 -0.7554 -0.1388\n",
       " -0.2289  0.9328 -0.2091  0.2623  0.4170  0.0594  0.2160  1.0048 -0.3688\n",
       "\n",
       "Columns 9 to 17 \n",
       "   0.9990  0.3992  0.1932  0.2181  0.2107  0.4811 -0.2953 -0.2539 -0.2851\n",
       "  0.4570 -0.3669 -0.3851  0.2134 -0.1099  0.1417  0.1676  0.5147  0.2696\n",
       " -0.7661 -0.0295  0.6203 -0.0213 -0.0976 -0.0533 -0.2855 -0.4774 -0.0755\n",
       "\n",
       "Columns 18 to 19 \n",
       "   0.3413 -0.1346\n",
       "  0.5598 -0.4964\n",
       " -0.0421  0.0872\n",
       "\n",
       "(2 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "   0.0161 -0.1211  0.0745 -0.2206  0.2140 -0.4045  0.3401 -0.1409 -0.3921\n",
       "  0.2569  0.0100 -0.5312  0.1919 -0.3224  0.2879 -0.8437 -0.3043 -0.0927\n",
       " -0.4678  0.5504 -0.2178  0.0871  0.4229 -0.1152  0.3696  0.5924 -0.4116\n",
       "\n",
       "Columns 9 to 17 \n",
       "   0.4874  0.0294  0.0649  0.0380  0.0434  0.2746  0.1123 -0.1601 -0.2634\n",
       "  0.2189 -0.1768  0.2585  0.2486  0.1496  0.1591  0.1251  0.2269  0.2071\n",
       "  0.0585 -0.2209  0.1596  0.1922 -0.3519 -0.1216 -0.1506 -0.4184 -0.2400\n",
       "\n",
       "Columns 18 to 19 \n",
       "   0.1261 -0.1853\n",
       "  0.2653 -0.5087\n",
       " -0.2783  0.1841\n",
       "\n",
       "(3 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "  -0.0919 -0.1672  0.2718 -0.2476  0.1141  0.0784  0.0885 -0.3356 -0.4941\n",
       "  0.0668 -0.0911 -0.4518  0.1981 -0.2915  0.0884 -0.5521 -0.0851  0.0677\n",
       " -0.2199  0.4572 -0.1632  0.3009  0.4157  0.2832  0.1833  0.2622 -0.6887\n",
       "\n",
       "Columns 9 to 17 \n",
       "   0.3578  0.1187 -0.0220  0.0637  0.2661  0.1766 -0.0258 -0.0429  0.1267\n",
       "  0.0962 -0.3203  0.1703  0.2904  0.2739  0.1050  0.1657  0.2872  0.0684\n",
       "  0.2365 -0.0368  0.1410  0.3674 -0.3663  0.1933 -0.4847 -0.6727 -0.2056\n",
       "\n",
       "Columns 18 to 19 \n",
       "   0.1838  0.0203\n",
       " -0.0710 -0.4002\n",
       " -0.0497  0.5382\n",
       "\n",
       "(4 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "  -0.1317 -0.3718  0.3927 -0.3639  0.0281  0.2283  0.3819 -0.5007 -0.1978\n",
       " -0.0591  0.0068 -0.1380  0.0584  0.1667  0.2373 -0.3456 -0.2018 -0.3490\n",
       " -0.0773  0.2528 -0.1214 -0.0404  0.5399 -0.1744  0.2522  0.4642 -0.0504\n",
       "\n",
       "Columns 9 to 17 \n",
       "   0.3977  0.3730 -0.2709 -0.2334 -0.2492 -0.0563 -0.0612  0.2628  0.3376\n",
       "  0.2049 -0.1599  0.0567  0.2247  0.1299  0.2497 -0.0397 -0.1628  0.1017\n",
       "  0.3385  0.0765  0.1538 -0.2478 -0.4766  0.1450  0.0760 -0.4583 -0.1639\n",
       "\n",
       "Columns 18 to 19 \n",
       "   0.1861 -0.0838\n",
       "  0.0542  0.0136\n",
       " -0.2339  0.2517\n",
       "[torch.FloatTensor of size 5x3x20]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
